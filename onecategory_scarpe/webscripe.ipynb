{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ed9308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1bbe0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.thehindu.com/\"\n",
    "CATEGORY_URL = \"https://www.thehindu.com/sport/\"\n",
    "CSV_FILENAME = \"the_hindu_articles.csv\"\n",
    "OUTPUT_DIR = \"article_metadata\"\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33973eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_links_to_csv():\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        response = requests.get(CATEGORY_URL, headers=HEADERS)\n",
    "        response.raise_for_status()  \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the category page: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    \n",
    "    article_links = set()\n",
    "    \n",
    "\n",
    "    for a_tag in soup.select('h3.title > a, .story-card-news > h3 > a, .latest-news-list > li > a'):\n",
    "        href = a_tag.get('href')\n",
    "        if href and href.startswith('https://www.thehindu.com/sport/'):\n",
    "            \n",
    "            if href.endswith('.ece'):\n",
    "                 article_links.add(href)\n",
    "\n",
    "    if not article_links:\n",
    "        print(\"Could not find any article links. The website structure might have changed.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(article_links)} unique article links.\")\n",
    "\n",
    "    # Writing the data to a CSV file\n",
    "    with open(CSV_FILENAME, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        # Write header\n",
    "        writer.writerow(['website', 'category', 'article_url'])\n",
    "        # Write data rows\n",
    "        for url in article_links:\n",
    "            writer.writerow(['The Hindu', 'Sport', url])\n",
    "            \n",
    "    print(f\"Successfully saved article links to '{CSV_FILENAME}'\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13c73751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Part 1: Scraping links from https://www.thehindu.com/sport/ ---\n",
      "Found 9 unique article links.\n",
      "Successfully saved article links to 'the_hindu_articles.csv'\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "scrape_links_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc53dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sanitize_filename(name):\n",
    "    name = re.sub(r'[\\\\/*?:\"<>|]', \"\", name)\n",
    "    name = name.replace(' ', '_')\n",
    "    return name[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7cb9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_metadata_from_csv():\n",
    "\n",
    "    if not os.path.exists(CSV_FILENAME):\n",
    "        print(f\"Error: CSV file '{CSV_FILENAME}' not found.\")\n",
    "        return\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    print(f\"JSON files will be saved in the '{OUTPUT_DIR}' directory.\")\n",
    "\n",
    "    with open(CSV_FILENAME, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        for i, row in enumerate(reader):\n",
    "            article_url = row['article_url']\n",
    "            print(f\"\\nProcessing article {i+1}: {article_url}\")\n",
    "\n",
    "            try:\n",
    "                response = requests.get(article_url, headers=HEADERS, timeout=10)\n",
    "                response.raise_for_status()\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"  -> Could not fetch article: {e}\")\n",
    "                continue\n",
    "\n",
    "            article_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # --- Extracting Metadata (Corrected Logic) ---\n",
    "            \n",
    "            # 1. Title (Robust check)\n",
    "            title_tag = article_soup.find('h1', class_='title')\n",
    "            if title_tag:\n",
    "                title = title_tag.get_text(strip=True)\n",
    "            else: # Fallback to meta tag if h1 is not found\n",
    "                meta_title_tag = article_soup.find('meta', {'property': 'og:title'})\n",
    "                title = meta_title_tag['content'] if meta_title_tag else \"Title not found\"\n",
    "\n",
    "            # 2. Summary (This was already correct)\n",
    "            summary_tag = article_soup.find('meta', {'name': 'description'})\n",
    "            summary = summary_tag['content'] if summary_tag else \"\"\n",
    "\n",
    "            # 3. Publish Date (This was already correct)\n",
    "            publish_date_tag = article_soup.find('meta', {'property': 'article:published_time'})\n",
    "            publish_date = publish_date_tag['content'] if publish_date_tag else \"\"\n",
    "\n",
    "            # 4. Article Image (This was already correct)\n",
    "            image_tag = article_soup.find('meta', {'property': 'og:image'})\n",
    "            article_image = image_tag['content'] if image_tag else \"\"\n",
    "\n",
    "            # 5. Image Credit\n",
    "            # Try multiple strategies to find image credit\n",
    "            image_credit = \"\"\n",
    "            # Strategy 1: Standard span\n",
    "            credit_span = article_soup.find('span', class_='credit')\n",
    "            if credit_span:\n",
    "               image_credit = credit_span.get_text(strip=True)\n",
    "\n",
    "            # Strategy 2: Inside figure > figcaption (used in some articles)\n",
    "            if not image_credit:\n",
    "               fig_caption = article_soup.select_one('figure figcaption')\n",
    "               if fig_caption:\n",
    "                  image_credit = fig_caption.get_text(strip=True)\n",
    "\n",
    "            # Strategy 3: Inside div with class \"caption\" or \"image-credit\"\n",
    "            if not image_credit:\n",
    "               caption_div = article_soup.find('div', class_=re.compile(r'(caption|image-credit)', re.I))\n",
    "               if caption_div:\n",
    "                  image_credit = caption_div.get_text(strip=True)\n",
    "\n",
    "            # 6. Article Content\n",
    "            article_content = \"\"\n",
    "            content_div = article_soup.find('div', id=re.compile(r'content-body-\\d+-\\d+'))\n",
    "            if content_div:\n",
    "             paragraphs = content_div.find_all('p')\n",
    "             article_content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
    "            else:  \n",
    "            # Fallback: try .articlebody or .content-area if present\n",
    "             alt_content = article_soup.find('div', class_=re.compile(r'(articlebody|content-area)'))\n",
    "            if alt_content:\n",
    "             paragraphs = alt_content.find_all('p')\n",
    "             article_content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
    "            else:\n",
    "             article_content = \"Article content not found.\"\n",
    "\n",
    "            # 7. Tags\n",
    "            # --- Tags Extraction ---\n",
    "            tags = \"\"\n",
    "\n",
    "            # Strategy 1: Article tags container\n",
    "            tags_div = article_soup.find('div', class_='article-tags-m')\n",
    "            if tags_div:\n",
    "               tags = \", \".join([a.get_text(strip=True) for a in tags_div.find_all('a')])\n",
    "\n",
    "            # Strategy 2: Fallback list structure\n",
    "            if not tags:\n",
    "               alt_tags = article_soup.select('ul.keywords > li > a, ul.article-keywords > li > a')\n",
    "               if alt_tags:\n",
    "                  tags = \", \".join([a.get_text(strip=True) for a in alt_tags])\n",
    "\n",
    "\n",
    "            # --- Assembling JSON data ---\n",
    "            metadata = {\n",
    "                'title': title,\n",
    "                'summary': summary,\n",
    "                'publish_date': publish_date,\n",
    "                'article_image': article_image,\n",
    "                'article_content': article_content,\n",
    "                'image_credit': image_credit,\n",
    "                'tags': tags,\n",
    "                'source_url': article_url\n",
    "            }\n",
    "            \n",
    "            # --- Saving the JSON file ---\n",
    "            timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "            sanitized_title = sanitize_filename(title)\n",
    "            json_filename = f\"thehindu_sport_{sanitized_title}_{timestamp}.json\"\n",
    "            json_filepath = os.path.join(OUTPUT_DIR, json_filename)\n",
    "            \n",
    "            with open(json_filepath, 'w', encoding='utf-8') as jsonfile:\n",
    "                json.dump(metadata, jsonfile, indent=4, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"  -> Successfully extracted and saved to '{json_filepath}'\")\n",
    "            \n",
    "    print(\"\\n--- All articles processed. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "531118e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Part 2: Extracting metadata for each article ---\n",
      "JSON files will be saved in the 'article_metadata' directory.\n",
      "\n",
      "Processing article 1: https://www.thehindu.com/sport/cricket/winning-at-edgbaston-will-be-one-of-my-happiest-memories-whenever-i-retire-shubman-gill/article69782541.ece\n",
      "  -> Successfully extracted and saved to 'article_metadata/thehindu_sport_Winning_at_Edgbaston_will_be_one_of_my_‘happiest_memories’_whenever_I_retire_Shubman_Gill_20250707163401.json'\n",
      "\n",
      "Processing article 2: https://www.thehindu.com/sport/cricket/vaughan-urges-inconsistent-zak-crawley-to-learn-from-shubman-gill/article69782533.ece\n",
      "  -> Successfully extracted and saved to 'article_metadata/thehindu_sport_Vaughan_urges_inconsistent_Zak_Crawley_to_learn_from_Shubman_Gill_20250707163402.json'\n",
      "\n",
      "Processing article 3: https://www.thehindu.com/sport/cricket/stokes-rues-missed-chances-after-crushing-loss/article69781250.ece\n",
      "  -> Successfully extracted and saved to 'article_metadata/thehindu_sport_Stokes_rues_missed_chances_after_crushing_loss_20250707163402.json'\n",
      "\n",
      "Processing article 4: https://www.thehindu.com/sport/motorsport/lando-norris-wins-thrilling-british-grand-prix-in-the-rain-to-cut-oscar-piastris-f1-lead/article69782075.ece\n",
      "  -> Successfully extracted and saved to 'article_metadata/thehindu_sport_Lando_Norris_wins_thrilling_British_Grand_Prix_in_the_rain_to_cut_Oscar_Piastri’s_F1_lead_20250707163403.json'\n",
      "\n",
      "Processing article 5: https://www.thehindu.com/sport/cricket/india-vs-england-second-test-in-birmingham-day-5-on-july-6-2025/article69779836.ece\n",
      "  -> Successfully extracted and saved to 'article_metadata/thehindu_sport_IND_vs_ENG_Akash_Deep’s_career-best_helps_India_break_Edgbaston_jinx_20250707163403.json'\n",
      "\n",
      "Processing article 6: https://www.thehindu.com/sport/cricket/for-akash-the-sky-is-the-limit/article69781211.ece\n",
      "  -> Successfully extracted and saved to 'article_metadata/thehindu_sport_For Akash,_the_sky_is_the_limitPremium_20250707163404.json'\n",
      "\n",
      "Processing article 7: https://www.thehindu.com/sport/tennis/wimbledon-alcaraz-sabalenka-reach-quarterfinals/article69782229.ece\n",
      "  -> Successfully extracted and saved to 'article_metadata/thehindu_sport_Wimbledon__Alcaraz,_Sabalenka_reach_quarterfinals_20250707163404.json'\n",
      "\n",
      "Processing article 8: https://www.thehindu.com/sport/cricket/icc-appoints-sanjog-gupta-as-its-new-ceo/article69782534.ece\n",
      "  -> Successfully extracted and saved to 'article_metadata/thehindu_sport_ICC_appoints_Sanjog_Gupta_as_its_new_CEO_20250707163405.json'\n",
      "\n",
      "Processing article 9: https://www.thehindu.com/sport/wimbledon-says-a-call-on-a-shot-that-landed-out-was-missed-because-the-electronic-system-was-off/article69781299.ece\n",
      "  -> Successfully extracted and saved to 'article_metadata/thehindu_sport_Wimbledon_says_a_call_on_a_shot_that_landed_out_was_missed_because_the_electronic_system_was_off_20250707163406.json'\n",
      "\n",
      "--- All articles processed. ---\n"
     ]
    }
   ],
   "source": [
    "scrape_metadata_from_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86e4a6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'article_metadata' directory created successfully.\n",
      "\n",
      "Found 9 JSON files. Here are the first 5:\n",
      " - thehindu_sport_ICC_appoints_Sanjog_Gupta_as_its_new_CEO_20250707163405.json\n",
      " - thehindu_sport_Winning_at_Edgbaston_will_be_one_of_my_‘happiest_memories’_whenever_I_retire_Shubman_Gill_20250707163401.json\n",
      " - thehindu_sport_Stokes_rues_missed_chances_after_crushing_loss_20250707163402.json\n",
      " - thehindu_sport_Wimbledon__Alcaraz,_Sabalenka_reach_quarterfinals_20250707163404.json\n",
      " - thehindu_sport_Wimbledon_says_a_call_on_a_shot_that_landed_out_was_missed_because_the_electronic_system_was_off_20250707163406.json\n",
      "\n",
      "'the_hindu_articles.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Check if the output directory and CSV file exist\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    print(f\"'{OUTPUT_DIR}' directory created successfully.\")\n",
    "    \n",
    "    # List a few JSON files from the directory\n",
    "    json_files = os.listdir(OUTPUT_DIR)\n",
    "    if json_files:\n",
    "        print(f\"\\nFound {len(json_files)} JSON files. Here are the first 5:\")\n",
    "        for filename in json_files[:5]:\n",
    "            print(f\" - {filename}\")\n",
    "    else:\n",
    "        print(\"The output directory is empty.\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error: '{OUTPUT_DIR}' directory not found.\")\n",
    "\n",
    "if os.path.exists(CSV_FILENAME):\n",
    "    print(f\"\\n'{CSV_FILENAME}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Error: '{CSV_FILENAME}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e35bd30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
